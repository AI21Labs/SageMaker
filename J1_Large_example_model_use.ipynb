{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Jurassic-1 Large on SageMaker through Model Packages\n",
    "\n",
    "This sample notebook shows you how to deploy Jurassic-1 Large using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites:\n",
    "1. Before running this notebook, please make sure you got this notebook from the model catalog on SageMaker AWS Management Console.\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**.\n",
    "1. This noebook is intended to work with **boto3 v1.25.4** or higher.\n",
    "\n",
    "## Contents:\n",
    "1. [Select model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Interact with the model](#B.-Interact-with-the-model)\n",
    "   3. [Create a few-shot prompt](#C.-Create-a-few-shot-prompt)\n",
    "   4. [Perform real-time inference](#D.-Perform-real-time-inference)\n",
    "   5. [Create the dashboard](#E.-Create-the-dashboard)\n",
    "   6. [Delete the endpoint](#F.-Delete-the-endpoint)\n",
    "3. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select to the model package\n",
    "Confirm that you recieved this notebook from model catalog on SageMaker AWS Management Console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_map = {\n",
    "    \"us-east-1\": \"arn:aws:sagemaker:us-east-1:416407187090:model-package/j1-large-dbf0249\",\n",
    "#     \"us-east-1\": \"arn:aws:sagemaker:us-east-1:865070037744:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"us-east-2\": \"arn:aws:sagemaker:us-east-2:057799348421:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"us-west-1\": \"arn:aws:sagemaker:us-west-1:382657785993:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"us-west-2\": \"arn:aws:sagemaker:us-west-2:594846645681:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"ca-central-1\": \"arn:aws:sagemaker:ca-central-1:470592106596:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"eu-central-1\": \"arn:aws:sagemaker:eu-central-1:446921602837:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"eu-west-1\": \"arn:aws:sagemaker:eu-west-1:985815980388:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"eu-west-2\": \"arn:aws:sagemaker:eu-west-2:856760150666:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"eu-west-3\": \"arn:aws:sagemaker:eu-west-3:843114510376:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"eu-north-1\": \"arn:aws:sagemaker:eu-north-1:136758871317:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"ap-southeast-1\": \"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"ap-southeast-2\": \"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"ap-northeast-2\": \"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"ap-northeast-1\": \"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"ap-south-1\": \"arn:aws:sagemaker:ap-south-1:077584701553:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\",\n",
    "#     \"sa-east-1\": \"arn:aws:sagemaker:sa-east-1:270155090741:model-package/j1-grande-dbf0249-66a2031af8f2315babd86234e01824e6\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker imports\n",
    "import json\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "import boto3\n",
    "\n",
    "# Use-case imports\n",
    "from tqdm import tqdm\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the version of boto3 - must be v1.25.4 or higher\n",
    "If you see a lower version number, pick another kernel to run the notebook, with Python 3.8 or above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install ai21 python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U \"ai21[SM]\"\n",
    "import ai21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in model_package_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "model_package_arn = model_package_map[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sagemaker_session = sage.Session()\n",
    "\n",
    "runtime_sm_client = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"j1-large\"\n",
    "\n",
    "content_type = \"application/json\"\n",
    "\n",
    "real_time_inference_instance_type = (\n",
    "    \"ml.g5.4xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployable model from the model package.\n",
    "model = ModelPackage(\n",
    "    role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Deploy the model\n",
    "predictor = model.deploy(1, real_time_inference_instance_type, endpoint_name=model_name, \n",
    "                         model_data_download_timeout=3600,\n",
    "                         container_startup_health_check_timeout=600,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Interact with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of Jurassic-1 Large as a smart auto-completion algorithm: it is very good at latching on to hints and patterns expressed in plain English, and generating text that follows the same patterns.\n",
    "\n",
    "Here are two words you should know:\n",
    "- **Prompt** - the input you provide to the model.\n",
    "- **Completion** - the output text the model generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter a simple prompt: \"To be or\", and let the model complete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ai21.Completion.execute(sm_endpoint=\"j1-large\",\n",
    "                                   prompt=\"To be or\",\n",
    "                                   maxTokens=4,\n",
    "                                   temperature=0,\n",
    "                                   numResults=1)\n",
    "\n",
    "print(response['completions'][0]['data']['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model identifies the beginning of a famous quote, and completes it correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Create a few-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to guide the model is to provide several examples of input-output pairs in the prompt. This establishes a pattern for the model to mimic. Then add the input for a query example and let the model complete it with an appropriate generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will create a prompt for analyzing hotel reviews written by users on Hotels.com, and performing freeform sentiment analysis: extracting the topics mentioned in the reviews and assigning a sentiment to each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we will build a few-shot prompt comprised of the following:\n",
    "\n",
    "1. Prefix with several examples. Each example contains the relevant inputs (a review) and the output (extracted topics and sentiments for this review). They are separated by \"##\".\n",
    "\n",
    "2. The query inputs. An unseen review for which we would like the model to output topics and sentiments. These should follow the same format of the inputs in the prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we collect some example data for the prompt prefix. For this, we will load 19 collected and annotated (by us) reviews for the Empire hotel in Hotels.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/hotels_sentimented.jsonl\", \"r\") as fp:\n",
    "    EXAMPLES_DATA = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_example(review, sentiment):   \n",
    "    example = \"Review:\\n\"\n",
    "    example += review\n",
    "    example += \"\\n\"\n",
    "    example += \"Extracted sentiment:\\n\"\n",
    "    example += sentiment\n",
    "    \n",
    "    return example\n",
    "\n",
    "SEPARATOR = \"\\n##\\n\"\n",
    "\n",
    "FEW_SHOT_PREFIX = SEPARATOR.join(\n",
    "    make_single_example(x['review'], json.dumps(x['sentiment'])) for x in EXAMPLES_DATA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we create a function to handle query inputs and create the full prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_review_analysis_prompt(review):\n",
    "    \"\"\"\n",
    "    Create a few-shot prompt to analysis reviews with Jurassic-1 Large \n",
    "    The prompt contains a preset sequence of examples followed by the query review\n",
    "    \"\"\"\n",
    "    return FEW_SHOT_PREFIX + SEPARATOR + make_single_example(review, '')  # keep the setniment blank and let the model generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reviews that were collected from Hotels.com\n",
    "# This is an array which contains 106 reviews\n",
    "with open(\"data/hotelsReviews.json\", \"r\") as fp:\n",
    "    reviews = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt is comprised from the following:\n",
    "1. Few-shot examples: pairs of input-output pairs. In this case, the input is a review and the output is a JSON dictionary containing the categories discussed in the review and their sentiment. They are separated by the following stop sequence: ##.\n",
    "For example:\n",
    "\n",
    "    *Review:\n",
    "    Was overall a good stay. We decided last minute to stay another night. No one really went out of their way to help us stay in the same room so we had to check out and go to a new room. It’s okay tho I understand some of those things are out of some hands.  It also worked out bec our first room’s AC didn’t seem to be working. If I had to complain about one thing, we got a “new” towel from the shower rack (folded) to dry off and the towel smelled like fresh vomit. That was pretty gross. Didn’t really have any other issues other than that. The hotel Is beautiful. \n",
    "    Extracted sentiment:\n",
    "    {\"Cleaning\": \"Negative\", \"AC\": \"Negative\", \"Room Quality\": \"Positive\", \"Service\": \"Negative\"}*\n",
    "\n",
    "2. A review to analyze. It should be written in the same format as all the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to put Jurassic-1 Large to work!\n",
    "\n",
    "Go through all the reviews in the list and call the model on each of them (with the appropriate prompt, of course).\n",
    "Collect all the sentiments in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "for review in tqdm(reviews):\n",
    "    prompt = create_review_analysis_prompt(review.strip())\n",
    "  \n",
    "    response = ai21.Completion.execute(sm_endpoint=\"j1-large\",\n",
    "                                       prompt=prompt,\n",
    "                                       maxTokens=150,\n",
    "                                       temperature=0,\n",
    "                                       stopSequences=['##'])\n",
    "\n",
    "    sentiments.append(response['completions'][0]['data']['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at an arbitrary review and sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 40\n",
    "print(\"Review:\\n\" + reviews[i])\n",
    "print(\"=======================\")\n",
    "print(\"Sentiments:\\n\" + sentiments[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Create the dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions take all the extracted categories and sentiments, and do the following:\n",
    "1. Gather all of the categories together, assigning a count of \"Positive\" or \"Negative\" to each category.\n",
    "2. Take the top count categories to display in the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, category_names = get_topK_categories_and_score(sentiments, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_dashboard(scores, category_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interested in learning more?\n",
    "Take a look at our [blog post](https://www.ai21.com/blog/build-a-dashboard-based-on-freeform-sentiment-analysis-of-hotel-reviews) to understand the process of building a good prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
